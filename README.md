# 강의
- Inflearn > 스프링부트로 직접 만들면서 배우는 대규모 시스템 설계(캐시 전략)

## 프로젝트 설정
### 레디스
- 실행
  - docker run --name cache-redis -d -p 6399:6379 redis:8.2.1
- 접속
  - docker exec -it cache-redis redis-cli
- 명령어 모음
  - 모든 키 조회
    - keys *
  - 데이터 생성
    - set mykey 'Hello Redis'
  - 데이터 조회
    - get mykey


## 개념정리
### 캐시
- 캐시란?
  - 더욱 빠른 저장소에 데이터를 보관하여, 해당 데이터에 빠르게 접근하는 기술
- 이점
  - 데이터 접근 속도 향상 
  - 저장소 부하 분산
    - ex. 레디스 같은 캐시를 사용하여 DB의 접근을 줄인다면, 부하가 분산되는 효과를 가져올 수 있다.
- Slow Storage와 Fast Storage
  - 데이터를 조회하기 위해서 대부분 Slow Storage에 접근한다.
    - 하지만 접근비용이 크고, 속도가 느리다.
    - 이를 해결하기 위해서 Slow Storage의 데이터를 Fast Storage에 캐시할 수 있다.
  - 모든 데이터를 Fast Storage에 저장하면 안되는가?
    - Fast Storage는 빠르지만 비싸다.
    - Slow Storage에 비해 안정성이 떨어진다.
    - 때문에 모든 데이터를 Fast Storage에 저장할 수는 없다.
    - 또한 파레토 법칙에 의하면, 모든 데이터를 Fast Storage에 저장할 필요도 없다.
      - 파레토 법칙
        - 전체 결과의 80%가 전체 원인의 20%에서 일어나는 현상
          - 전체 조회 트래픽의 대부분(약 80%)은 소수의 데이터(약 20%)에 집중된다. 
          - 자주 조회되는 핵심 데이터가 반복해서 읽힌다.
        - 따라서 일부 데이터만 캐시해도 효율적으로 동작한다.
- 캐시의 종류
  - Local Cache
    - 각 서버 애플리케이션 내부 메모리에 캐시
    - 매우 빠름
    - 서버 메모리 부담, 재시작 시 휘발, 데이터 일관성 관리 어려움(서버 애플리케이션 간 동기화 문제)
    - 대표 라이브러리 : Caffeine, Ehcache 등
  - Remote Cache(Global Cache)
    - 별도 서버 메모리에 캐시
    - 네트워크 통한 접근 필요하므로 Local Cache 보다 느림
    - 별도 서버 메모리에 저장하기 때문에 Local Cache 보다 큰 데이터를 장기간 저장할 수 있음
    - 여러 서버 애플리케이션이 동일한 저장소에 접근하므로 데이터 일관성 관리 유리
    - 대표 솔루션 : Redis, Memcached 등 인메모리 데이터베이스 및 라이브러리

### Redis
- Redis의 특징
  - In-memory Database
  - 고성능
  - NoSQL(Not Only SQL)
    - 관계형 데이터베이스와 달리 정해진 스키마가 없고, 유연한 데이터 모델 사용
  - 키-값 저장소
  - 다양한 자료 구조를 지원
    - String, List, Set, Sorted Set, Hash 등
  - TTL(Time To Live) 지원
    - 일정 시간이 지나면 데이터 자동 삭제
  - Single Thread
    - Redis는 단일 스레드에서 순차적으로 처리한다.
    - 각 명령어가 순차적/원자적으로 처리되므로, 동시성 문제를 해결하는데 유리하다.
  - Pub/Sub – 발행/구독 시스템
  - 데이터 백업 지원
    - 메모리는 휘발성 저장소지만, Redis에서 데이터를 디스크에 백업하는 방법도 제공(AOF, RDB)
  - Redis Cluster
    - 확장성, 부하 분산, 고가용성 등을 위한 분산 시스템 구성 방법 제공
- Redis Cluster
  - Redis의 수평 확장(Scale-Out)을 위한 기술
    - 여러 개의 Redis 서버가 클러스터를 이루면서 분산 시스템 제공
  - 고가용성, 안정성, 확장성, 부하 분산 등 다양한 이점 제공
  - 대규모 데이터와 트래픽을 다루는데 유리
  - Sharding
    - 데이터를 여러 노드에 분산하는 기술
    - 새로운 노드 추가 시 자동으로 데이터가 분산된다.
    - 확장성, 부하 분산 제공
  - Replication
    - 데이터를 복제하는 기술
    - Master 노드에 데이터를 쓰면, Replica 노드에 데이터를 복제한다.
    - 고가용성, 안정성, 부하 분산 제공

### 스프링의 캐시 애노테이션
- @Cacheable
  - Look-Aside 방식으로 동작한다.
    - 애플리케이션이 캐시를 직접 조회하고 관리한다.
    - 데이터를 읽을 때 캐시를 우선 조회하고, 데이터가 없으면(Cache Miss) DB에서 조회한 뒤 캐시에 저장하는 방식이다.
  - 동작방식
    - 캐시에서 데이터를 찾았으면, 메서드를 수행하지 않고 데이터를 반환한다.
    - 캐시에서 데이터를 찾지 못하였으면, 메서드를 수행하고 반환 값을 캐시에 저장한다.
  - Data Source의 데이터를 조회하는 메서드에 적용할 수 있다.
    - 응답된 데이터를 캐시에 저장하여, Data Source 접근을 줄일 수 있음
- @CachePut
  - 메서드를 수행하고, 반환 값을 캐시에 갱신한다.
  - 원본 데이터를 생성/수정하는 메서드에 적용할 수 있다.
- @CacheEvict
  - 메서드를 수행하고, 캐시에서 데이터를 삭제한다.
  - 원본 데이터를 삭제하는 메서드에 적용할 수 있다.
- 한계점
  - 대부분 상황에서는 캐시 애노테이션(Look-Aside 전략)으로 충분할 수 있지만, 한계점도 존재한다.
    - Cache Penetration
      - 캐시에 존재하지 않는 데이터를 반복적으로 조회하여 매번 DB까지 접근하게 되는 문제.
    - Cache Stampede
      - 캐시가 만료되는 순간 대량의 요청이 동시에 DB로 몰려 과부하가 발생하는 문제.
    - Hot Key
      - 특정 키에 요청이 집중되어 해당 캐시 서버나 DB에 부하가 쏠리는 현상.

### Cache Penetration
- Cache Penetration 이란?
  - 캐시 관통 
    - 캐시를 관통해서 Data Source로 요청이 전파되는 현상
  - 캐시 관통 흐름
    - 클라이언트가 특정 키로 조회 요청 
    - 캐시에 없음 (Cache Miss)
    - DB 조회 → 결과 없음 
    - 캐시에 저장할 값이 없으니 그대로 종료 
    - 같은 요청이 계속 들어오면 매번 DB까지 조회됨
  - 문제점
    - 캐시에서 트래픽이 제한되지 못하고, Data Source로 전파
    - Data Source가 모든 부하를 빠르게 받아내지 못한다면, 예기치 못한 장애 위험
  - 해결방법
    - Null Object Pattern
    - Bloom Filter

### Null Object Pattern
- null 데이터도 캐시에 저장  
  - 조회 결과가 없는 경우에도 `null` 대신 Null Object를 캐시에 저장하여, 동일한 요청이 반복될 때 DB 재조회 없이 캐시에서 바로 처리
- @Cacheable 메소드는 Null Object 반환  
  - 캐시 대상 메소드는 `null`을 반환하지 않고, 항상 객체를 반환하도록 설계하여 캐시 일관성을 유지
- Null Object의 기준 정의  
  - 식별자(id)가 `null`인 객체를 Null Object로 약속하여, 정상 데이터와 구분 가능하도록 설계
- 부가 정보 포함 가능  
  - 단순히 '없음'만 표현하지 않고, 상태나 메타 정보를 포함한 객체로 확장 가능
- 실패 사유 캐싱  
  - 데이터 접근 실패 원인을 함께 저장하여 재조회 없이 캐시에서 바로 판단 가능
- 유연한 사용자 응답  
  - 실제로 존재하지 않는 데이터인지, 비공개 데이터인지 등의 사유를 캐시에서 조회하여 사용자에게 안내 가능

### Null Object Pattern의 단점
- 정상 데이터 생성 시 캐시 불일치  
  - Null Object로 캐시된 키에 대해 이후 Data Source에 정상 데이터가 생성되면, 캐시에는 여전히 '데이터 없음'으로 판단될 수 있음
- 짧은 TTL의 한계  
  - 만료 시간을 매우 짧게 가져가더라도 만료까지의 지연은 존재하며, 캐시 히트율이 떨어질 수 있음
- 캐시 강제 만료의 복잡성  
  - Data Source에 데이터가 생성되는 시점에 Null Object로 캐시된 키를 직접 만료시켜야 함
- 불필요한 캐시 처리  
  - 실제로 캐시 만료가 필요 없는 데이터까지 함께 처리해야 하므로 불필요한 작업이 증가
- 분산 시스템에서의 한계  
  - 예를 들어 A 서비스가 B 서비스의 응답을 받아 Null Object를 캐시한 경우, B의 데이터 생성 시점을 A가 알기 위한 추가 메커니즘이 필요
- 무작위 요청으로 인한 캐시 메모리 고갈 위험  
  - 존재하지 않는 데이터에 대해 무작위 파라미터 요청이 들어오면 경우의 수가 사실상 무한해지며, 그 결과 다수의 Null Object가 캐시에 저장됨  
  - 캐시 메모리가 빠르게 소모되어 정상적인 데이터 캐싱이 어려워질 수 있음  
  - IP 차단, 요청 제한, 짧은 TTL, 캐시 서버 증설 등으로 완화는 가능하지만, 트래픽 폭주나 악의적 요청 상황에서는 근본적인 해결책이 되기 어려움
- 캐시 정책의 한계  
  - LRU 등의 만료 정책을 사용하더라도 캐시 히트율 저하와 공격 패턴 대응의 어려움은 여전히 존재

### Bloom Filter
- Null Object Pattern의 단점 보완
  - 메모리를 효율적으로 사용하면서도 Cache Penetration 문제를 해결할 수 있는 대안
- 개념
  - 데이터의 실제 값을 저장하지 않고, 존재 여부만을 빠르게 판단하기 위한 확률적 자료 구조
- 해시 기반 구조
  - 여러 개의 독립적인 해시 함수를 사용하여 데이터를 비트 배열에 매핑
- 메모리 효율성
  - 비트 배열만 사용하므로 대량의 데이터를 매우 적은 메모리로 관리 가능
- 확률적 판단 특성
  - Bloom Filter에 없다고 판단된 데이터는 확실히 존재하지 않음  
  - Bloom Filter에 있다고 판단된 데이터는 실제로 없을 수도 있음
- False Positive 가능성
  - 해시 충돌로 인해 존재하지 않는 데이터를 있다고 잘못 판단할 수 있음
- 기본 동작 방식
  - 데이터 삽입 시 해시 함수 결과에 해당하는 비트를 1로 설정  
  - 데이터 조회 시 하나라도 0이면 '확실히 없음', 모두 1이면 '있을지도 모름'
  - 예시
    - A 데이터 추가
      - h1(A) → 0번째 인덱스 체크
      - h2(A) → 2번째 인덱스 체크
      - h3(A) → 5번째 인덱스 체크
    - B 데이터 추가
      - h1(B) → 2번째 인덱스 체크
      - h2(B) → 4번째 인덱스 체크
      - h3(B) → 7번째 인덱스 체크
    - C 데이터 조회 (존재하지 않는 데이터)
      - h1(C) → 1번째 인덱스 확인 (0)
      - h2(C) → 3번째 인덱스 확인 (0)
      - h3(C) → 6번째 인덱스 확인 (0)
      - 하나라도 0이므로 Bloom Filter에 입력된 적 없음
    - D 데이터 조회 (False Positive 발생)
      - h1(D) → 2번째 인덱스 확인 (1)
      - h2(D) → 5번째 인덱스 확인 (1)
      - h3(D) → 7번째 인덱스 확인 (1)
      - 실제로는 입력되지 않았지만, 모든 비트가 1이므로 존재할 수도 있다고 판단
- 삭제 불가 특성
  - 어떤 데이터가 비트를 설정했는지 알 수 없기 때문에 기본 Bloom Filter는 삭제를 지원하지 않음
- Cache Penetration 방지 방식
  - Bloom Filter에서 없다고 판단된 요청은 캐시나 DB까지 접근하지 않고 즉시 차단 가능
  - Bloom Filter 적용 전과 후 비교
    - 기존 캐시 구조
      - Redis에 없으면 DB를 조회하는 방식
    - Bloom Filter 적용 후 구조
      - Bloom Filter → Redis → DB
    - 요청 흐름
      - Bloom Filter에서 '없음'으로 판단되면
        - Redis 및 DB 조회를 수행하지 않음
      - '있을 수도 있음'으로 판단된 경우에만 Redis 조회 진행
- 실무 활용 예시
  - 블랙리스트 필터링, 중복 데이터 검사, 조회 비용이 큰 데이터의 사전 필터링

### Bloom Filter의 한계
- 데이터 수 증가에 따른 오차율 증가
  - 초기 설계 시 가정한 데이터 수(n)를 초과하면 오차율이 급격히 증가 (mightContain_whenBloomFilterAddedTooManyData 테스트코드 참고)
    - 비트 배열이 대부분 1로 채워지며 False Positive가 빈번해짐
- 한계에 대한 보완 방식
  - Split / Sharding / Sub Filter는 이 문제를 해결하는 것이 아님
    - Bloom Filter의 구조적 특성상 오차율 증가 자체는 피할 수 없음
  - 다만 데이터 분산 또는 추가 검증을 통해
    - 오차율이 급격히 증가하는 시점을 늦추거나
    - False Positive로 인한 DB 접근 비용을 줄이는 보완 전략으로 사용됨

### Bloom Filter - Split
- Redis 비트맵 크기 제한
  - Redis의 비트맵은 최대 2^32 비트(512MB)까지만 지원
  - 최대 offset은 2^32 - 1로 제한됨
  - 입력 데이터 수(n)와 오차율(p)에 따라 이보다 더 큰 Bloom Filter가 필요할 수 있음
- 문제 상황
  - 필요한 Bloom Filter 크기가 3 × 2^32 비트인 경우
    - Redis 제약으로 인해 단일 Key로는 생성 불가능
    - 거대한 Bloom Filter를 하나의 비트맵으로 표현할 수 없음
- Split 개념
  - Split은 Bloom Filter 분할 전략
  - 최대 크기인 2^32 비트짜리 Bloom Filter 여러 개로 나누어 구성
  - 물리적으로는 여러 개의 작은 Bloom Filter
  - 논리적으로는 하나의 거대한 Bloom Filter처럼 동작
- Split 방식 예시
  - Split 1 : 0 ~ (2^32 - 1) 비트
  - Split 2 : 2^32 ~ (2 × 2^32 - 1) 비트
  - Split 3 : (2 × 2^32) ~ (3 × 2^32 - 1) 비트
- 애플리케이션 처리 방식
  - 해시 결과로 나온 비트 위치(offset)를 기준으로
    - 어느 Split에 속하는지 계산
    - 해당 Split의 Bloom Filter Key에 접근
  - 각 Split은 서로 다른 Redis Key로 관리
- Split 전략의 효과
  - Redis의 비트맵 크기 제한을 우회하여
    - 논리적으로 매우 큰 Bloom Filter 구성 가능
  - Split Key가 서로 다르기 때문에
    - Redis Cluster 환경에서는 샤딩 전략에 따라
    - 각 Split이 여러 노드에 자연스럽게 분산될 수 있음

### Bloom Filter - Split의 한계
- 단일 필터 구조
  - 현재 Bloom Filter는 단일하게 관리됨
    - Split 전략은 비트 배열을 논리적으로 나눈 것일 뿐
      - 논리적으로는 여러 Split으로 분리되어 보이지만
      - 전체적으로는 하나의 Bloom Filter를 구성함
- 조회 방식의 구조적 한계
  - Bloom Filter 조회 시
    - 하나의 값에 대해 여러 해시 함수가 생성한 인덱스를 모두 확인해야 함
      - 해당 인덱스가 어느 Split에 속하는지와 무관하게
      - 결과적으로 모든 Split을 대상으로 조회가 발생
- 부하 집중 문제
  - 논리적으로만 분리되어 있고 물리적으로는 단일 구조이기 때문에
    - 모든 조회·쓰기 트래픽이 하나의 Bloom Filter에 집중될 수 있음
      - 읽기 트래픽 증가
      - 쓰기 트래픽 증가
      - 저장 공간 사용량 증가
- 한계에 대한 대응
  - 이러한 단일 구조로 인한 부하 집중을 완화하기 위한 방법으로
    - Bloom Filter를 물리적으로 분산하는 Sharding 기법을 적용할 수 있음

### Bloom Filter - Sharding
- Sharding은 하나의 Bloom Filter에 집중되던 데이터를 여러 개의 Bloom Filter로 분산하여 관리하는 전략
- 단일 Bloom Filter에 집중되던 부하를 여러 Shard로 분산할 수 있음
  - 읽기 트래픽 분산
  - 쓰기 트래픽 분산
  - 저장 공간 분산
- Sharding은 애플리케이션 레벨에서 적용할 수 있는 전략
  - Redis 내부 기능이 아닌, Key 설계 방식으로 구현
- 적용 방식
  - 하나의 Bloom Filter 대신 N개의 Bloom Filter를 사용
  - 각 Bloom Filter는 서로 다른 Key를 가짐
    - 예: bloom-filter:0, bloom-filter:1, bloom-filter:2
  - 조회 및 삽입 시 Shard Index를 계산하여 특정 Bloom Filter만 접근
    - shardIndex = hash(value) % N
- 구조 변화에 따른 효과
  - 단일 Bloom Filter 사용 시
    - Client → Bloom Filter (key=bloom-filter)
    - 단일 Key에 트래픽과 저장 공간이 집중됨
      - Hot Key 문제 발생 가능
  - Sharding 적용 시
    - Client → Bloom Filter (key=bloom-filter:0)
    - Client → Bloom Filter (key=bloom-filter:1)
    - Client → Bloom Filter (key=bloom-filter:2)
    - Key가 분리되면서 Redis Cluster에서도 노드별로 자연스럽게 분산
      - 트래픽 집중 완화
      - 저장 공간 분산
      - Hot Key 문제 해결
- Split 전략과의 차이
  - Split 전략
    - Bloom Filter를 물리적으로 여러 조각으로 분할
    - 논리적으로는 하나의 Bloom Filter로 동작
    - 모든 Split을 항상 조회해야 함
  - Sharding 전략
    - Bloom Filter를 물리적·논리적으로 모두 분산
    - 특정 데이터는 특정 Shard의 Bloom Filter만 조회
  - 서로 다른 전략이므로 중첩 적용 가능
    - 예: Sharding으로 분산한 뒤, 각 Shard 내부를 Split으로 다시 분할 (`강의에서는 중첩 적용하였음`)
- 메모리 효율 관점의 장점
  - Bloom Filter는 데이터 수(n)가 증가할수록 오차율(p)이 급격히 1에 수렴하는 특성을 가짐
  - 단일 Bloom Filter로 대량 데이터를 관리할 경우
    - 낮은 오차율을 유지하기 위해 매우 큰 메모리가 필요
      - 예: n=100,000,000, m=512MB
  - Sharding 적용 시
    - 데이터를 여러 Bloom Filter로 나누어 관리
      - 예: 1억 개 데이터를 10개의 Shard로 분산
      - 각 Shard는 더 적은 데이터 수를 가짐
    - 상대적으로 작은 Bloom Filter 크기로도 충분히 낮은 오차율 유지 가능
    - 결과적으로 전체 메모리 사용량을 줄일 수 있음
- 해시 함수 개수(k)와의 관계
  - 오차율(p)을 낮추기 위해 해시 함수 수(k)를 늘릴 수도 있음
    - 하지만 k가 증가할수록 조회·삽입 시 연산 비용 증가
  - Sharding을 적용하면
    - 데이터 수 자체가 분산되므로
    - k를 과도하게 늘리지 않아도 목표한 오차율을 만족하는 경우가 많음
- 정리
  - Sharding은 Bloom Filter의 부하를 분산하는 데 초점을 둔 전략
  - Hot Key 문제를 해결하고 트래픽과 저장 공간을 고르게 분산할 수 있음
  - 메모리 효율과 성능을 동시에 개선할 수 있는 현실적인 접근 방식
